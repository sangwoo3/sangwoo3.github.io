[{"authors":["admin"],"categories":null,"content":"Hi! My name is Sangwoo Cho or 조상우 (in Korean) and I am a 5th-year Ph.D. student in the Computer Science Department at the University of Central Florida. My research interests include Computer Vision and Natural Language Processing with Machine Learning. I am currently under the supervision of Dr. Hassan Foroosh in the CIL and Dr. Fei Liu in the UCF NLP group.\nMy current research focuses on two folds: human action recognition and text summarization of news articles. More specifically, I have been working on video-based (a sequence of images or keypoints) human action recognition. It is important to extract spatial and temporal information carefully as videos contain both information. Another research topic I have been working on is multi-document summarization, which seeks to take a cluster of multiple articles and create a summary. This requires special process to focus on the most important information, while avoiding redundancy that is common in multi-document clusters.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sangwoo3.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi! My name is Sangwoo Cho or 조상우 (in Korean) and I am a 5th-year Ph.D. student in the Computer Science Department at the University of Central Florida. My research interests include Computer Vision and Natural Language Processing with Machine Learning. I am currently under the supervision of Dr. Hassan Foroosh in the CIL and Dr. Fei Liu in the UCF NLP group. My current research focuses on","tags":null,"title":"","type":"authors"},{"authors":["Sangwoo Cho","Kaiqiang Song","Chen Li","Dong Yu","Hassan Foroosh","Fei Liu"],"categories":null,"content":"","date":1600056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600056000,"objectID":"1dc8a999b01740467cff13cabd902455","permalink":"https://sangwoo3.github.io/publication/emnlp20/","publishdate":"2020-09-14T00:00:00-04:00","relpermalink":"/publication/emnlp20/","section":"publication","summary":"In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of texts. The method allows summaries to be understood in context to prevent any summarizer from distorting the original meaning, of which abstractive summarization can fall short. In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid any confusion. Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights. To show the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets. Our analysis provides further evidence that highlighting is a promising avenue of research toward future summarization.","tags":[],"title":"Better Highlighting: Creating Sub-Sentence Summary Highlights","type":"publication"},{"authors":["Sangwoo Cho","Muhammad Hasan Maqbool","Fei Liu","Hassan Foroosh"],"categories":null,"content":"","date":1583038800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583038800,"objectID":"d0887a19ef44e362a806f5992c9179c8","permalink":"https://sangwoo3.github.io/publication/wacv20/","publishdate":"2020-03-01T00:00:00-05:00","relpermalink":"/publication/wacv20/","section":"publication","summary":"Skeleton-based action recognition has recently attracted a lot of attention. Researchers are coming up with new approaches for extracting spatio-temporal relations and making considerable progress on large-scale skeleton-based datasets. Most of the architectures being proposed are based upon recurrent neural networks (RNNs), convolutional neural networks (CNNs) and graph-based CNNs. When it comes to skeleton-based action recognition, the importance of long term contextual information is central which is not captured by the current architectures. In order to come up with a better representation and capturing of long term spatio-temporal relationships, we propose three variants of Self-Attention Network (SAN), namely, SAN-V1, SAN-V2 and SAN-V3. Our SAN variants has the impressive capability of extracting high-level semantics by capturing long-range correlations. We have also integrated the Temporal Segment Network (TSN) with our SAN variants which resulted in improved overall performance. Different configurations of Self-Attention Network (SAN) variants and Temporal Segment Network (TSN) are explored with extensive experiments. Our chosen configuration outperforms state-of-the-art Top-1 and Top-5 by 4.4% and 7.9% respectively on Kinetics and shows consistently better performance than state-of-the-art methods on NTU RGB+D.","tags":[],"title":"Self-Attention Network for Skeleton-based Human Action Recognition","type":"publication"},{"authors":["Sangwoo Cho","Chen Li","Dong Yu","Hassan Foroosh","Fei Liu"],"categories":null,"content":"","date":1572753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572753600,"objectID":"4b499b5957043cfa087e18042d0b8c60","permalink":"https://sangwoo3.github.io/publication/emnlp19/","publishdate":"2019-11-03T00:00:00-04:00","relpermalink":"/publication/emnlp19/","section":"publication","summary":"Emerged as one of the best performing techniques for extractive summarization, determinantal point processes select the most probable set of sentences to form a summary according to a probability measure defined by modeling sentence prominence and pairwise repulsion. Traditionally, these aspects are modelled using shallow and linguistically informed features, but the rise of deep contextualized representations raises an interesting question of whether, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences.","tags":["Workshop"],"title":"Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations","type":"publication"},{"authors":["Sangwoo Cho","Logan Lebanoff","Hassan Foroosh","Fei Liu"],"categories":null,"content":"","date":1562126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562126400,"objectID":"d9bff58e6f04415f66f36d0c105b94c5","permalink":"https://sangwoo3.github.io/publication/acl19/","publishdate":"2019-07-03T00:00:00-04:00","relpermalink":"/publication/acl19/","section":"publication","summary":"The most important obstacles facing multi-document summarization include excessive redundancy in source descriptions and the looming shortage of training data. These obstacles prevent encoder-decoder models from being used directly, but optimization-based methods such as determinantal point processes (DPPs) are known to handle them well. In this paper we seek to strengthen a DPP-based method for extractive multi-document summarization by presenting a novel similarity measure inspired by capsule networks. The approach measures redundancy between a pair of sentences based on surface form and semantic meanings. We show that our DPP system with improved similarity measure performs competitively, outperforming strong summarization baselines on benchmark datasets. Our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions.","tags":["Oral"],"title":"Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization","type":"publication"},{"authors":["Sangwoo Cho","Hassan Foroosh"],"categories":null,"content":"","date":1543726800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543726800,"objectID":"1998ca75724366aa21ad04ebf2255761","permalink":"https://sangwoo3.github.io/publication/accv18/","publishdate":"2018-12-02T00:00:00-05:00","relpermalink":"/publication/accv18/","section":"publication","summary":"The video based CNN works have focused on effective ways to fuse appearance and motion networks, but they typically lack utilizing temporal information over video frames. In this work, we present a novel spatio-temporal fusion network (STFN) that integrates temporal dynamics of appearance and motion information from entire videos. The captured temporal dynamic information is then aggregated for a better video level representation and learned via end-to-end training. The spatio-temporal fusion network consists of two set of Residual Inception blocks that extract temporal dynamics and a fusion connection for appearance and motion features. The benefits of STFN are: (a) it captures local and global temporal dynamics of complementary data to learn video-wide information; and (b) it is applicable to any network for video classification to boost performance. We explore a variety of design choices for STFN and verify how the network performance is varied with the ablation studies. We perform experiments on two challenging human activity datasets, UCF101 and HMDB51, and achieve the state-of-the-art results with the best network.","tags":[],"title":"Spatio-Temporal Fusion Networks for Action Recognition","type":"publication"},{"authors":["Sangwoo Cho","Hassan Foroosh"],"categories":null,"content":"","date":1520827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520827200,"objectID":"867f8eb0d4c0d6428ff0846670d054c4","permalink":"https://sangwoo3.github.io/publication/wacv18/","publishdate":"2018-03-12T00:00:00-04:00","relpermalink":"/publication/wacv18/","section":"publication","summary":"In this work, we present a method to represent a video with a sequence of words, and learn the temporal sequencing of such words as the key information for predicting and recognizing human actions. We leverage core concepts from the Natural Language Processing (NLP) literature used in sentence classification to solve the problems of action prediction and action recognition. Each frame is converted into a word that is represented as a vector using the Bag of Visual Words (BoW) encoding method. The words are then combined into a sentence to represent the video, as a sentence. The sequence of words in different actions are learned with a simple but effective Temporal Convolutional Neural Network (T-CNN) that captures the temporal sequencing of information in a video sentence. We demonstrate that a key characteristic of the proposed method is its low-latency, i.e. its ability to predict an action accurately with a partial sequence (sentence). Experiments on two datasets, UCF101 and HMDB51 show that the method on average reaches 95% of its accuracy within half the video frames. Results, also demonstrate that our method achieves compatible state-of-the-art performance in action recognition (i.e. at the completion of the sentence) in addition to action prediction.","tags":[],"title":"A Temporal Sequence Learning for Action Recognition and Prediction","type":"publication"},{"authors":["Sangwoo Cho","Enrique Dunn","Jan-Michael Frahm"],"categories":null,"content":"","date":1395633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1395633600,"objectID":"6e805febb4f6c5bc675a2808e3d7c1ca","permalink":"https://sangwoo3.github.io/publication/wacv14/","publishdate":"2014-03-24T00:00:00-04:00","relpermalink":"/publication/wacv14/","section":"publication","summary":"We address the problem of online relative orientation estimation from streaming video captured by a sky-facing camera on a mobile device. Namely, we rely on the detection and tracking of visual features attained from cloud structures. Our proposed method achieves robust and efficient operation by combining realtime visual odometry modules, learning based feature classification, and Kalman filtering within a robustness-driven data management framework, while achieving framerate processing on a mobile device. The relatively large 3D distance between the camera and the observed cloud features is leveraged to simplify our processing pipeline. First, as an efficiency driven optimization, we adopt a homography based motion model and focus on estimating relative rotations across adjacent keyframes. To this end, we rely on efficient feature extraction, KLT tracking, and RANSAC based model fitting. Second, to ensure the validity of our simplified motion model, we segregate detected cloud features from scene features through SVM classification. Finally, to make tracking more robust, we employ predictive Kalman filtering to enable feature persistence through temporary occlusions and manage feature spatial distribution to foster tracking robustness. Results exemplify the accuracy and robustness of the proposed approach and highlight its potential as a passive orientation sensor.","tags":[],"title":"Rotation estimation from cloud tracking","type":"publication"}]