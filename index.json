[{"authors":["admin"],"categories":null,"content":"Hi! My name is Sangwoo Cho or 조상우 (in Korean) and I am a 4th-year Ph.D. student in the Computer Science Department at the University of Central Florida. My research interests include Computer Vision, Video Action Recognition, Natural Language Processing, Automatic Text Summarization, and Deep Learning. I am currently under the supervision of Dr. Hassan Foroosh in the CIL and Dr. Fei Liu in the UCF NLP group.\nMy current research focuses on two folds: human action recognition and extractive text summarization of news articles. More specifically, I am working on video and skeleton based human action recognition, which takes a video or key points of a human body and predicts human activities. It is important to extract spatial and temporal information carefully as videos contain both information. Another research topic I am working on is multi-document summarization, which seeks to take a cluster of multiple articles and create one summary. This requires special process to focus on the most important information, while avoiding redundancy that is common in multi-document clusters.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sangwoo3.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi! My name is Sangwoo Cho or 조상우 (in Korean) and I am a 4th-year Ph.D. student in the Computer Science Department at the University of Central Florida. My research interests include Computer Vision, Video Action Recognition, Natural Language Processing, Automatic Text Summarization, and Deep Learning. I am currently under the supervision of Dr. Hassan Foroosh in the CIL and Dr. Fei Liu in the UCF NLP group.","tags":null,"title":"","type":"authors"},{"authors":["Sangwoo Cho","Chen Li","Dong Yu","Hassan Foroosh","Fei Liu"],"categories":null,"content":"","date":1572753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572753600,"objectID":"4b499b5957043cfa087e18042d0b8c60","permalink":"https://sangwoo3.github.io/publication/emnlp19/","publishdate":"2019-11-03T00:00:00-04:00","relpermalink":"/publication/emnlp19/","section":"publication","summary":"Emerged as one of the best performing techniques for extractive summarization, determinantal point processes select the most probable set of sentences to form a summary according to a probability measure defined by modeling sentence prominence and pairwise repulsion. Traditionally, these aspects are modelled using shallow and linguistically informed features, but the rise of deep contextualized representations raises an interesting question of whether, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences.","tags":["Workshop"],"title":"[Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations](https://arxiv.org/pdf/1910.11411.pdf)","type":"publication"},{"authors":["Sangwoo Cho","Logan Lebanoff","Hassan Foroosh","Fei Liu"],"categories":null,"content":"","date":1562126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562126400,"objectID":"d9bff58e6f04415f66f36d0c105b94c5","permalink":"https://sangwoo3.github.io/publication/acl19/","publishdate":"2019-07-03T00:00:00-04:00","relpermalink":"/publication/acl19/","section":"publication","summary":"The most important obstacles facing multi-document summarization include excessive redundancy in source descriptions and the looming shortage of training data. These obstacles prevent encoder-decoder models from being used directly, but optimization-based methods such as determinantal point processes (DPPs) are known to handle them well. In this paper we seek to strengthen a DPP-based method for extractive multi-document summarization by presenting a novel similarity measure inspired by capsule networks. The approach measures redundancy between a pair of sentences based on surface form and semantic meanings. We show that our DPP system with improved similarity measure performs competitively, outperforming strong summarization baselines on benchmark datasets. Our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions.","tags":["Oral"],"title":"Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization","type":"publication"},{"authors":["Sangwoo Cho","Hassan Foroosh"],"categories":null,"content":"","date":1543726800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543726800,"objectID":"1998ca75724366aa21ad04ebf2255761","permalink":"https://sangwoo3.github.io/publication/accv18/","publishdate":"2018-12-02T00:00:00-05:00","relpermalink":"/publication/accv18/","section":"publication","summary":"The video based CNN works have focused on effective ways to fuse appearance and motion networks, but they typically lack utilizing temporal information over video frames. In this work, we present a novel spatio-temporal fusion network (STFN) that integrates temporal dynamics of appearance and motion information from entire videos. The captured temporal dynamic information is then aggregated for a better video level representation and learned via end-to-end training. The spatio-temporal fusion network consists of two set of Residual Inception blocks that extract temporal dynamics and a fusion connection for appearance and motion features. The benefits of STFN are: (a) it captures local and global temporal dynamics of complementary data to learn video-wide information; and (b) it is applicable to any network for video classification to boost performance. We explore a variety of design choices for STFN and verify how the network performance is varied with the ablation studies. We perform experiments on two challenging human activity datasets, UCF101 and HMDB51, and achieve the state-of-the-art results with the best network.","tags":[],"title":"Spatio-Temporal Fusion Networks for Action Recognition","type":"publication"},{"authors":["Sangwoo Cho","Hassan Foroosh"],"categories":null,"content":"","date":1520827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520827200,"objectID":"867f8eb0d4c0d6428ff0846670d054c4","permalink":"https://sangwoo3.github.io/publication/wacv18/","publishdate":"2018-03-12T00:00:00-04:00","relpermalink":"/publication/wacv18/","section":"publication","summary":"In this work, we present a method to represent a video with a sequence of words, and learn the temporal sequencing of such words as the key information for predicting and recognizing human actions. We leverage core concepts from the Natural Language Processing (NLP) literature used in sentence classification to solve the problems of action prediction and action recognition. Each frame is converted into a word that is represented as a vector using the Bag of Visual Words (BoW) encoding method. The words are then combined into a sentence to represent the video, as a sentence. The sequence of words in different actions are learned with a simple but effective Temporal Convolutional Neural Network (T-CNN) that captures the temporal sequencing of information in a video sentence. We demonstrate that a key characteristic of the proposed method is its low-latency, i.e. its ability to predict an action accurately with a partial sequence (sentence). Experiments on two datasets, UCF101 and HMDB51 show that the method on average reaches 95% of its accuracy within half the video frames. Results, also demonstrate that our method achieves compatible state-of-the-art performance in action recognition (i.e. at the completion of the sentence) in addition to action prediction.","tags":[],"title":"A Temporal Sequence Learning for Action Recognition and Prediction","type":"publication"}]